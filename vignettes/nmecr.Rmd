---
title: "Introduction to nmecr"
author: 
  - Mrinalini Sharma^[msharma@kw-engineering.com]
  - David Jump^[djump@kw-engineering.com]
  - Devan Johnson^[johnson@kw-engineering.com]

date: "October 1, 2019"
output: pdf_document
vignette: >
  %\VignetteIndexEntry{Introduction to nmecr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

devtools::load_all()
library(dplyr)
library(lubridate)
library(ggplot2)
library(tidyr)
```

The `nmecr` package was written to integrate the past efforts of the energy efficiency industry to streamline meter-based Measurement & Verification of EE projects. It brings together simple linear regression with outside air temperature , the change point models (from ASHARE 1040-RP), and the Time-of-Week and Temperature model (developed by the Lawrence Berkeley National Laboratory) and builds enhancements upon these by accurately predicting the energy use profiles of facilities with multiple operating modes.  

#### Normalized Metered Energy Consumption (NMEC)

Meter-based energy efficiency programs are proliferating across the globe. This proliferation is influenced by the widespread availability of high frequency energy use measurements from new metering technology as well as advancements in statistical regression and other empirical energy data modeling methodologies. Program administrators may report savings as the overall reduction in normalized metered energy consumption (NMEC). This method to determine savings is based on analysis of meter data collected prior to and after energy efficiency and conservation measures have been installed. Referred to as advanced measurement and verification (M&V) by the industry, this time-granular data and updated modeling methods provide several advantages over other methods used to quantify the benefits of energy efficiency:

* It reliably determines the actual savings achieved at the meter

*	It provides fast feedback on the facility's energy performance and savings progress

*	It enables identification and troubleshooting of issues that prevent savings realization
 
The `nmecr` package streamlines the application of the NMEC approach by enabling the:

* Management of high frequency, high volume data.

* Execution of advanced, state-of-the-art time-series data modeling algorithms.

* Comprehensive assessment of the validity of energy data models in specific applications.

* Quantification of uncertainties and risks associated with the energy savings projections in accordance with ASHRAE Guideline 14.

## Data

`nmecr` includes two datasets: `temp` and `eload`. These contain two years (03/01/2012 - 02/28/2015) of outside air temperature data and energy use data from a commercial building in North America. Energy conservation measures were installed between 03/01/2013 and 02/28/2014 in this building and so, we will use the first year (03/01/2012 - 02/01/2013) as the pre-implementation dataset and the third year (03/01/2014 - 02/28/2015) as the post-implementation dataset.

`temp` and `eload` are data frames with the two variables each. When using `nmecr` functions, ensure that the column headers of your datasets are the same as those shown below.


```{r temp, results='markup', echo=FALSE}
head(temp, n=5)
```

```{r eload, results='markup', echo=FALSE}
head(eload, n=5)
```

## Aggregating Energy Use and Temperature data

nmecr contains three main data manipulation functions:

* `agg_eload_temp_df_to_hourly()`

* `agg_eload_temp_df_to_daily()`

* `agg_eload_temp_df_to_monthly()`

These functions perform two tasks:

1. Aggregating data from small time intervals to large time intervals.   

1. Combining `eload` and `temp` datasets into one data frame. 

Note that the `temp` and `eload` do not need to necessarily have the same time interval. For example, you can input 15-min electric consumption data and hourly temperature data to `agg_eload_temp_df_to_daily()` to generate aggregated electric consumption and temperature data at the daily level. In fact, data can be made available in any of the following time intervals and aggregated to hourly, daily, and/or monthly intervals as needed: less than 15-minutes, 15-minutes, hourly, daily, and monthly time intervals.

```{r daily_energy_use, results='markup'}
daily_energy_use <- agg_eload_temp_df_to_daily(eload_data = eload, temp_data = temp,
                                                 data_is_quantity = T,
                                                 balancepoint_temp = 65)
```

```{r daily_energy_use head, results='markup', echo=FALSE}
head(daily_energy_use, n = 5)
```

## Separating pre-implementation and post-implementation datasets

Once the `eload` and `temp` datasets are combined into one data frame, the next step is to create the pre- and post-implementation data frames. You can choose any method to do so. We recommend using the dplyr and the lubridate package.

```{r daily_pre, results='hide'}
daily_pre <- daily_energy_use %>%
  filter(time < lubridate::mdy("3/1/2013"))

daily_post <- daily_energy_use %>%
  filter(time > lubridate::mdy("2/28/2014"))
```

## Data Modeling

`nmecr` contains four data modeling algorithms. These create the data models, report on the models' goodness-of-fit metrics, provide details on the model residuals' skewness and kurtosis, and predict using the developed model when given a prediction dataset. 

* `model_with_TOWT()`: Time-of-Week & Temperature and Time-Only algorithms

* `model_with_CP()`: 3-Parameter Heating, 3-Parameter Cooling, 4-Parameter, and 5-Parameter algorithms

* `model_with_SLR()`: Simple Linear Regression algorithm

* `model_with_HDD_CDD()`: Heating Degree Day only, Cooling Degree Day only, and a combination of Heating Degree Day and Cooling Degree Day algorithms

`model_with_TOWT()` and `model_with_SLR()` have three additional arguments: has_operating_modes, train_operating_mode_data, pred_operating_mode_data. We use these arguments for facilities that have more than one operating mode. Schools, for example, have a distinct operating mode in the summer. For usage of these arguments, please refer to the function documentation (type ?model_with_SLR or ?model_with_TOWT in the console.)

### Modeling with Time of Week and Temperature

Simple Linear Regression and its derivatives (changepoint models and HDD/CDD models) are the most popular data modeling algorithms in the energy efficiency community. The Time-of-Week and Temperature (TOWT) model, developed by Lawrence Berkeley National Laboratory, has improved upon the predictive capabilties of these algorithms by adding an additional variable as a regressor: Time-of-Week. While early implementations of TOWT required the occupancy schedule of the facility to be an input to the algorithm, the implementation below gleans this information from the energy use profile itself. As a result, the only independent variable input required by the algorithm is temperature, which is made available in the daily_pre and the daily_post data frames already. 

```{r TOWT}
TOWT_results <- model_with_TOWT(training_data = daily_pre, data_interval = "Daily", 
                                data_units = "kWh")
```

The `TOWT_results` object contains all information pertaining to the developed model, some of which is shown below:


Model Goodness of Fit:

```{r TOWT results, echo = FALSE}
TOWT_results$goodness_of_fit
```

Model Results:

```{r training data model, fig.width=7, fig.height=3, echo = FALSE}
TOWT_results$training_data$time <- lubridate::mdy_hm(TOWT_results$training_data$time)

df <- TOWT_results$training_data %>%
  dplyr::select(time, eload, yfit) %>%
  tidyr::gather(key = "variable", value = "value", -time)

ggplot2::ggplot(df, aes(x = time, y = value)) +
  geom_line(aes(color = variable), size = 1) +
  scale_color_manual(values = c("darkorange", "dodgerblue3")) +
  xlab("Date")+
  scale_y_continuous(name = "Energy Consumption and Model Fit", labels = scales::comma)+
  theme_minimal()

ggplot2::ggplot(TOWT_results$training_data, aes(x = eload, y = yfit)) +
  geom_point()

```

Model Residuals:

```{r residual histogram, fig.width=7, fig.height=3, echo = FALSE}
df <- TOWT_results$training_data

ggplot2::ggplot(df, aes(resi)) +
  geom_histogram(stat = "bin", bins= 30) +
  theme_minimal()
```

```{r TOWT residuals, echo = FALSE}
TOWT_results$normality_metrics[, c(2,4)]
```

The California Public Utilities Commission requires savings to be detectable above model variations. `nmecr` interprets this using ASHRAE Guideline 14 - 2014's formulation for savings uncertainty, which relates the savings uncertainty to the model goodness of fit metric CV(RMSE), the confidence level, the amount of savings, the amount of data used to develop the model, and the amount of data required to report savings. It includes a correction when autocorrelation is present (which occurs mainly in models developed from daily and hourly data). LBNL has shown this uncertainty formulation with correction for autocorrelation underestimates the savings uncertainty. More work on this issue is needed. Until a better formulation is available, `nmecr` uses ASHRAE's method only as an estimation. 

`nmecr` enables the user to specify the level of savings expected, then estimates the savings uncertainty based on a few different confidence levels (68%, 90%, 95%, and 99.7%), the model's CV(RMSE), and the number of data points in the post-installation dataset. If a post-implementation dataset is not available, it assumes the count of post-implementation datapoints to be the same as the baseline dataset.  As uncertainties are stated as the savings estimate 'plus or minus' the savings uncertainty, savings are considered detectable when the saving uncertainty is 50% or less. This information can be obtained using the `calculate_savings_uncertainty()` function:


```{r savings_unc @ 10, results='markup'}
savings_uncertainty_10 <- calculate_savings_uncertainty(modeled_data_obj = TOWT_results, 
                                                        savings_percent = 10)
```
```{r savings_unc @ 10 90, results='markup', echo=FALSE}
savings_uncertainty_10$uncertainty_summary[2, c(2,3)]

```

The uncertainty for 10% savings at a confidence level of 90% is 14.2% for this model. The corresponding minimum savings needed for this project are 169,406 kWh

### Predicting energy use using Time-of-Week and Temperature algorithm

We can create model predictions by simply adding an argument with the prediction dataframe to the `model_with_towt()` function:

```{r TOWT predictions, results='markup'}
TOWT_predictions <- model_with_TOWT(training_data = daily_pre, prediction_data = daily_post, 
                                    data_interval = "Daily", data_units = "kWh")

```

```{r post implementation energy use, fig.width=7, fig.height=3, echo = FALSE}
TOWT_predictions$post_implementation_data$time <- lubridate::mdy_hm(TOWT_predictions$post_implementation_data$time)

df_pred <- TOWT_predictions$post_implementation_data %>%
  dplyr::select(time, eload, pred_eload) %>%
  tidyr::gather(key = "variable", value = "value", -time)

ggplot2::ggplot(df_pred, aes(x = time, y = value)) +
  geom_line(aes(color = variable), size = 1) +
  scale_color_manual(values = c("darkorange", "dodgerblue3")) +
  xlab("Date")+
  scale_y_continuous(name = "Energy Consumption and Model Fit", labels = scales::comma)+
  theme_minimal()
```

The predictions can further be summarized to calculate 'Avoided Energy Use' for the project:

### Avoided Energy Use and Savings Uncertainty

Avoided energy use is the savings achieved in the post-installation period. It is calculated from the difference in the adjusted baseline energy use and the post-installation period measured energy use. The adjusted baseline energy use is determined from the baseline model's predictions after inputting the post-installation period temperature and time data.

```{r avoided energy use, results='markup'}

avoided_energy_use <- calculate_avoided_energy_use(modeled_data_obj = TOWT_predictions)
```

```{r avoided energy use results, results='markup', echo=FALSE}
avoided_energy_use$savings_summary

```

Actual savings uncertainties for the avoided energy use are estimated using the same function as before, but using the actual number of points in the post-installation period. 

```{r actual savings uncertainty, results='markup'}

actual_savings_uncertainty <- calculate_savings_uncertainty(modeled_data_obj = 
                                                              TOWT_predictions, 
                                                            savings_percent = 
                                                            avoided_energy_use$pct_savings)
```  
```{r actual savings uncertainty 90, results='markup', echo = FALSE}
actual_savings_uncertainty$uncertainty_summary[2, 2]

```

```{r avoided energy use plot, fig.width=7, fig.height=3, echo=FALSE}

savings_df <- avoided_energy_use$savings_df
cumulative_savings <- cumsum(savings_df$savings)

cumulative_savings_df <- dplyr::bind_cols("time" = savings_df$time, "cusum_savings" = cumulative_savings)

ggplot2::ggplot(cumulative_savings_df, aes(x = time, y = cusum_savings)) +
  geom_area() + 
  scale_y_continuous(name = "Cumulative Savings", labels = scales::comma) +
  theme_minimal()

```

### Normalized Savings

Normalized Savings for an energy efficiency project can be calculated using the functions described above by assigning a normalized temperature dataset as the prediction dataset. Remember to name the dataframe columns as: `time` and `temp`.  
